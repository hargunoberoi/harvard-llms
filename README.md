# Session notes

Following repository contains the code to train a small transformer based model demonstrated during the course [Applying LLMs in practice](https://sites.harvard.edu/harvard-seas-llm/directory/hargun-oberoi/?cp-dir-id=431) at Harvard SEAS.

You can also find below the resources that I discussed during my presentations.

### Useful Links

1. [Tiktokenizer](https://tiktokenizer.vercel.app/) - This was used to visualize the tokens.
2. [Together model inference](https://together.ai) - This was used to make an inference with open source models like llama.
3. [Huggingface Inference playground](https://huggingface.co/spaces/huggingface/inference-playground) - This was used to make an inference with a larger set of open-source models.


### Key papers

1. [Llama 3 paper](https://arxiv.org/abs/2407.21783)
2. [Instruct GPT paper](https://arxiv.org/abs/2203.02155)
3. [Deepseek R1 paper](https://arxiv.org/abs/2501.12948)

### Acknowledgements

The ChesterGPT architecture is based on the NanoGPT project by Andrej Karpathy. You can find that [here](https://github.com/karpathy/nanoGPT.git)

### License

MIT

If any questions, please raise an issue here on this repository.
